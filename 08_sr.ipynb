{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('learn_rl': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7bb3d8ac6e6250e6a3299c466ac42ab4b558d814d5e6d7a1e5bef797a01d4de1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Introduction\n",
    "\n",
    "Equations are adapted from Russek et al. 2017 and Momennejad 2020. These papers, like Sutton & Barto, express successor states as $s_{t+1}$ or $s_{t'}$ (i.e., t-prime). Here, equations instead adopt the perspective of the agent, who subjectively experiences a current state $s_t$ and \"remembers\" the previous state $s_{t-1}$. The successor state is therefore expressed as $s_t$.\n",
    "\n",
    "Code is adapted from Ida Momennejad's GitHub repository on [predictive representations](https://github.com/idamomen/predictive_representations).\n",
    "\n",
    "Russek, E. M., Momennejad, I., Botvinick, M. M., Gershman, S. J., & Daw, N. D. (2017). Predictive representations can link model-based reinforcement learning to model-free mechanisms. PLOS Computational Biology, 13(9), e1005768. doi:10.1371/journal.pcbi.1005768\n",
    "\n",
    "Momennejad, I. (2020). Learning Structures: Predictive Representations, Replay, and Generalization. Current Opinion in Behavioral Sciences, 32, 155-166. doi:10.1016/j.cobeha.2020.02.017"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import igraph as ig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "source": [
    "# The Successor Representation\n",
    "\n",
    "Given that an agent starts out in state $X$, how often can it expect to end up in state $Y$? This is the kind of question addressed by the Successor Representation (SR), which can be learned through experience using standard temporal difference (TD) methods in the reinforcement learning (RL) framework.\n",
    "\n",
    "As usual, the RL problem could be tackled as a prediction problem (learning the value of states), or a control problem (learning the value of actions).\n",
    "\n",
    "## The SR matrix\n",
    "\n",
    "If we're solving the prediction problem, this requires us to generate an $s \\times s$ matrix, where $s$ is the number of states, rows indicate starting out in $X$, and columns indicate a potential future state $Y$.\n",
    "\n",
    "If we're solving the control problem, this requires us to generate an $s \\times sa$ matrix, where $a$ is the number of actions available at each state. For simplicity, this implementation assumes that the same actions are permissible in every state, though this is not a strictly necessary assumption to make."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_sr(n_states, n_actions=None):\n",
    "    \"\"\"\n",
    "    Initialize a fresh set of matrices for learning the Successor Representation.\n",
    "        Arguments:\n",
    "            n_states:\n",
    "                Number of states in the environment\n",
    "            n_actions (optional):\n",
    "                Number of actions that can be taken in each state\n",
    "        Returns:\n",
    "            successor_matrix:\n",
    "                Starting in state X, how often can I expect to end up in state Y?\n",
    "    \"\"\"\n",
    "    \n",
    "    if n_actions is None:\n",
    "        # Build successor matrix for learning counts in states\n",
    "        successor_matrix = np.zeros((n_states, n_states))\n",
    "        weight_vector = np.zeros(n_states)\n",
    "    else:\n",
    "        # Build successor matrix for learning counts in state-action pairs\n",
    "        successor_matrix = np.zeros((n_states, n_states * n_actions))\n",
    "        weight_vector = np.zeros(n_states * n_actions)  # this might not be right, check this later\n",
    "\n",
    "    return successor_matrix, weight_vector"
   ]
  },
  {
   "source": [
    "## SR update rule\n",
    "\n",
    "Once a state transition has occurred, how do we update the SR matrix using TD learning?\n",
    "\n",
    "For reference, the standard TD value-learning update rule is:\n",
    "\n",
    "$ V(s_{t-1}) \\leftarrow V(s_{t-1}) + \\alpha [r_{t} + \\gamma V(s_{t}) - V(s_{t-1})]$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "In the SR matrix, we are interested in counts, not values. So we can simply replace the value terms with counts:\n",
    "\n",
    "$ M(s_{t-1}) \\leftarrow M(s_{t-1}) + \\alpha [\\boldsymbol{1}(s_t) + \\gamma M(s_t) - M(s_{t-1})]$,\n",
    "\n",
    "where $\\boldsymbol{1}$ is a function producing a vector of $0$s except for a single $1$ at the index $s_t$ (aka, a *one-hot* vector).\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "As usual, the RL problem can either be tackled as a prediction problem (learning the value of states), or a control problem (learning the value of actions). The above formulations are for learning state counts only. We can extend this into the Q-learning framework so that we can instead learn counts for actions *within* states:\n",
    "\n",
    "$ H(s_{t-1}a_{t-1}) \\leftarrow H(s_{t-1} a_{t-1}) + \\alpha [\\boldsymbol{1}(s_t a_t) + \\gamma H(s_t a_t) - H(s_{t-1} a_{t-1})] $\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "We relaxed the notation a little bit for simplicity, and to emphasize the similarity with the standard value-learning update rule. In reality, the matrix $M$ is constructed *for a particular policy* $\\pi$ and should technically be written as $M^{\\pi}$. Additionally, in implementation, *an entire row* of the SR gets replaced, as shown here:\n",
    "\n",
    "$ M^{\\pi}(s_{t-1}, \\space :) \\leftarrow M^{\\pi}(s_{t-1}, \\space :) + \\alpha [\\boldsymbol{1}(s_t) + \\gamma M^{\\pi}(s_t, \\space :) - M^{\\pi}(s_{t-1}, \\space :)]$\n",
    "\n",
    "$ H(s_{t-1}a_{t-1}, \\space :) \\leftarrow H(s_{t-1} a_{t-1}, \\space :) + \\alpha [\\boldsymbol{1}(s_t a_t) + \\gamma H(s_t a_t, \\space :) - H(s_{t-1} a_{t-1}, \\space :)] $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sr(sr_matrix, agent_params, agent_location):\n",
    "    \"\"\"\n",
    "    Given a transition from state X to state Y, update the SR with the latest count of expected visits to Y (and its successors) from X.\n",
    "        Arguments:\n",
    "            sr_matrix:\n",
    "                The most recent SR matrix. Expects a numpy *float!* array.\n",
    "            agent_params:\n",
    "                Tuple of free parameters dictating the agent's learning.\n",
    "                    [0] = alpha (learning rate)\n",
    "                    [2] = gamma (temporal discounting)\n",
    "            agent_location:\n",
    "                Tuple of the agent's current and previous 'location' occupancy.\n",
    "                    [0] = previous state / state-action pair\n",
    "                    [1] = current state / state-action pair\n",
    "        Returns:\n",
    "            sr_matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # For ease of reading code\n",
    "    alpha = agent_params[0]\n",
    "    gamma = agent_params[2]\n",
    "    loc_prev = agent_location[0]\n",
    "    loc_now = agent_location[1]\n",
    "    \n",
    "    # Create the one-hot (row) vector that signals what the current state is\n",
    "    this_onehot = np.zeros(np.shape(sr_matrix)[0])\n",
    "    this_onehot[loc_now] = 1\n",
    "\n",
    "    # Compute the temporal difference prediction error\n",
    "    delta = this_onehot + (gamma * sr_matrix[loc_now, :]) - sr_matrix[loc_prev, :]\n",
    "\n",
    "    # Now update the SR matrix\n",
    "    sr_matrix[loc_prev, :] = sr_matrix[loc_prev, :] + (alpha * delta)\n",
    "    \n",
    "    # Return updated SR matrix\n",
    "    return sr_matrix"
   ]
  },
  {
   "source": [
    "# Value-learning\n",
    "\n",
    "For the purpose of estimating the value of states, it's not sufficent to represent the environment's structure (well, more accurately, expectations about future state occupancies). You must also know where to find rewards, and must integrate this reward knowledge into your knowledge about the environment.\n",
    "\n",
    "## Function approximation and generalization\n",
    "\n",
    "The major limitation of standard RL is that an optimal policy can only be found if an agent explores every part of the state space (as otherwise, it can't accurately estimate the value of all states in the environment). In complex environments, this is all but impossible. Moreover, we have the intuition that an agent should value states more similarly if the states are more similar to each other. Therefore, an agent needs to be able to *infer* the value of states even if it cannot directly visit them, and we believe that this inference process should incorporate information about states' similarity to each other. In other words, an agent must devise a way to *approximate* a given function (such as a value function), which allows it to make such inferences by generalizing its past experience. This is known, sensibly, as function approximation.\n",
    "\n",
    "As per Sutton & Barto 1998, section 8.3, there's a fairly straightforward linear method for doing this. Imagine that every state can be described using some \"features.\" For example, in the children's game \"the floor is lava\", the objective is to avoid touching the floor. The optimal strategy is therefore to hop through an environment on top of tables and chairs. A relevant feature for this environment might be knowing whether each state's elevation is higher than the elevation of the floor. This creates a binary feature variable, where $0 = \\text{same elevation}$ and $1 = \\text{greater elevation}$. Even if the entire state space is not explored (indeed, it'd be in one's best interests *not* to visit state spaces where the floor is lava), knowing the feature space is sufficient for generalization. We can represent feature information in a vector, where each element represents each state's value in feature space.\n",
    "\n",
    "How do we then translate knowledge about features into an approximation of the value function? A different way to ask this question is to ask how to *weight* features so that we can estimate the value of unseen states. If this sounds like linear regression to you, then you're exactly right. The general form of a linear regression is $ \\hat{Y} = \\beta_1Predictor_1 + \\beta_2Predictor_2 + ... + \\beta_nPredictor_n $ (for exposition, intercept/error terms are omitted). In the exact same way, our feature vector $x$ is multiplied against a weighting vector $w$ to produce the value estimate $\\hat{V}$. If there are $n$ features that are used to describe the value function, then we would have a linear sum of features in the general form $ \\hat{V} = x_1 w_1 + x_2 w_2 + ... + x_n w_n $ for each state.\n",
    "\n",
    "## Using the SR for function approximation\n",
    "\n",
    "Dayan 1992 was the first to suggest that the standard TD value function can be reparameterized in this way, producing the equation:\n",
    "\n",
    "$ V^\\pi(s_{t-1}) = \\sum_{s_t} M^\\pi(s_{t-1}, s_t) \\sum_a \\pi(a|s_t) R(s_t, a) $\n",
    "\n",
    "For conceptual clarity, this equation can be rewritten to emphasize that there are two distinct components:\n",
    "\n",
    "$ V^\\pi(s_{t-1}) = \\sum_{s_t} M^\\pi(s_{t-1}, s_t) w(s_t) $\n",
    "\n",
    "We've already examined the SR component, expressed as $M^\\pi(s_t, s_{t+1})$. This can be conceptualized as a feature matrix (where each row is a feature vector), such that each \"feature\" being encoded by the agent is an expectation of ending up in a given successor state starting from their current state.\n",
    "\n",
    "If we've already defined a feature vector $M$, the second component we need is a weighting vector $w$. If we care about reward knowledge, that weighting vector needs to represent the expected reward in a successor state (given that many different actions could be taken, which might result in different amounts of reward). We can express this as the combination of knowledge about the one-step reward in successor state $s_t$ given some action that is taken $R(s_t, a)$, which is averaged over all possible actions that could be taken in the successor state. Therefore, we'd like $w = \\sum_a \\pi(a|s_t) R(s_t, a)$.\n",
    "\n",
    "In this formulation, the SR is known as a \"basis function\" for the value function. It's a fancy way of saying that you can linearly combine the SR with other knowledge in order to produce the value function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value(sr_matrix, weight_vector, state):\n",
    "    \"\"\"\n",
    "    The values of states X and Y are not stored in the usual state-value matrix, but must be computed by combining the SR matrix and weight vector.\n",
    "        Arguments:\n",
    "            sr_matrix:\n",
    "                The most recent SR matrix. Expects a numpy *float!* array.\n",
    "            weight_vector:\n",
    "                The most recent weight vector. Expects a 1-D numpy array.\n",
    "            state:\n",
    "                A state / state-action pair. Scalar.\n",
    "            \n",
    "        Returns:\n",
    "            value_state\n",
    "    \"\"\"\n",
    "    \n",
    "    value_state = np.inner(sr_matrix[state, :], weight_vector)\n",
    "\n",
    "    return value_state"
   ]
  },
  {
   "source": [
    "## Learning weights through experience\n",
    "\n",
    "Weights can be estimated from experience through standard TD learning:\n",
    "\n",
    "$ w_{new}(:) \\leftarrow w_{old}(:) + \\alpha \\delta M^\\pi(s_{t-1}, :) $\n",
    "\n",
    "where $\\alpha$ is a learning rate, $M$ is the SR matrix, and the prediction error is $ \\delta = R(s_{t-1}, a) + \\gamma V(s_t) - V(s_{t-1}) $.\n",
    "\n",
    "## Scaling (optional read)\n",
    "\n",
    "In implementation, when we pull out a particular row of the SR matrix to update $w$ (i.e., the feature vector corresponding to the transition from $s_{t-1}$ to $s_t$), we'll want to use a *scaled* version of $M$. Why is this? (Much of this section is either adapted or outright lifted from personal correspondence with Evan Russek, who very kindly walked me through the logic.)\n",
    "\n",
    "In the standard TD update equation, we want our learning rate $\\alpha$ to reflect what proportion of a predictor error is used for value updating. This looks like:  \n",
    "$ \\Delta V = V(s_t) - V(s_{t-1}) $\n",
    "\n",
    "We want updates in $w$ to have the same proportional effect on value updating as $\\alpha$, so we can use scaling as a \"trick\" for doing this. But what would happen if we didn't use scaling? Recall that value can be computed by taking the dot product of the relevant SR row and the reward vector, which makes the value equation:  \n",
    "$ \\Delta V = M(s_t)w_{new} - M(s_t)w_{old} $\n",
    "\n",
    "This can be algebraically rearranged as:  \n",
    "$ \\Delta V = M(s_t) [w_{new} - w_{old}] $\n",
    "\n",
    "We then replace $w_{new}$ from the weight-learning update equation:  \n",
    "$ \\Delta V = M(s_t) [w_{old} + \\alpha \\delta M(s_{t-1}) - w_{old}] $\n",
    "\n",
    "This simplifies to:  \n",
    "$ \\Delta V = M(s_t) [\\alpha \\delta M(s_{t-1})] $\n",
    "\n",
    "And to really drive home the point, we can rearrange the terms:  \n",
    "$ \\Delta V = \\alpha \\delta M(s_t) M(s_{t-1}) $\n",
    "\n",
    "So we see there's a problem here, which is that the proportional update from $\\delta$ to $\\Delta V$ is not really dictated by $\\alpha$, but rather by $ \\alpha M(s_t) M(s_{t-1}) $. This completely messes with our weights, and functionally prevents the SR agent from ever converging upon a stable value estimate.\n",
    "\n",
    "To solve this, we can (temporarily!) scale $M(s_{t-1})$ in the following way:  \n",
    "$ M(s_{t-1})_{scaled} \\leftarrow M(s_{t-1}) \\space \\oslash \\space \\sum  M(s_{t-1}) M(s_{t-1}) $,  \n",
    "where $\\oslash$ denotes element-wise division (Hadamard division).\n",
    "\n",
    "If all this notation looks like chickenscratch, here's a more intuitive description of what scaling does. First, find the sum-of-squares of the SR row you're interested in. Then, divide each element of the SR row by that sum-of-squares. That takes us from $ M(s_t) M(s_{t-1}) \\alpha \\delta $ to $ M(s_t) M(s_{t-1}) \\alpha \\delta $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(sr_matrix, agent_params, agent_location, weight_vector, reward):\n",
    "    \"\"\"\n",
    "    Given a transition from state X to state Y, and the associated reward, update the weight matrix so that the SR can be mapped to the value function.\n",
    "        Arguments:\n",
    "            sr_matrix:\n",
    "                The most recent SR matrix. Expects a numpy *float!* array.\n",
    "            agent_params:\n",
    "                Tuple of free parameters dictating the agent's learning.\n",
    "                    [0] = alpha (learning rate)\n",
    "                    [2] = gamma (temporal discounting)\n",
    "                        In principle, the agent could have a different alpha/gamma for updating weights vs the SR.\n",
    "                        But, for simplicity, we assume that they're the same.\n",
    "            agent_location:\n",
    "                Tuple of the agent's current and previous 'location' occupancy.\n",
    "                    [0] = previous state / state-action pair\n",
    "                    [1] = current state / state-action pair\n",
    "            weight_vector:\n",
    "                The most recent weight vector. Expects a 1-D numpy array.\n",
    "            reward:\n",
    "                Scalar reflecting the amount of reward observed when transitioning from X to Y.\n",
    "        Returns:\n",
    "            weight_vector\n",
    "    \"\"\"\n",
    "\n",
    "    # For ease of reading code\n",
    "    alpha = agent_params[0]\n",
    "    gamma = agent_params[2]\n",
    "    loc_prev = agent_location[0]\n",
    "    loc_now = agent_location[1]\n",
    "\n",
    "    # Compute state values\n",
    "    value_loc_prev = compute_value(sr_matrix, weight_vector, loc_prev)\n",
    "    value_loc_now = compute_value(sr_matrix, weight_vector, loc_now)\n",
    "    \n",
    "    # Compute prediction error\n",
    "    delta = reward + (gamma * value_loc_now) - value_loc_prev\n",
    "\n",
    "    # Compute feature-scaled SR\n",
    "    scaled_sr = sr_matrix[loc_prev, :] / np.sum(np.square(sr_matrix[loc_prev, :]))\n",
    "\n",
    "    # Use prediction error to update the weight vector\n",
    "    weight_vector = weight_vector + (alpha * delta * scaled_sr)\n",
    "\n",
    "    return weight_vector"
   ]
  },
  {
   "source": [
    "# Replay (SR-Dyna)\n",
    "\n",
    "If an agent is only capable of learning from direct experience, it would require a *lot* of trial-and-error to form a stable/accurate estimate of state/action values. What if an agent were additionally able to *simulate* experience, and to learn from that simulated experience? This is the key idea behind the Dyna-RL architecture, developed by Sutton & Barto. Interestingly, there is evidence that the brain \"replays\" experience to aid in memory consolidation.\n",
    "\n",
    "## Recency mechanism\n",
    "\n",
    "Which events should be \"selected\" for replay? One simple solution is to sample from past experience, with a bias for sampling the most recently-experienced transitions. This is conceptually similar to the eligibility trace method, except that eligibility traces operate directly on state/action value estimates during \"online\" direct experience. Instead, we'll give the agent a memory system that stores a history of what transitions it has recently experienced, which is only tapped when performing \"offline\" updates to the SR matrix.\n",
    "\n",
    "There are different ways that a recency mechanism could be implemented. In past published work, Ida Momennejad and colleagues have favored an exponentially weighted function where the last $n$ transitions are ordered from $[0:n-1]$, divided by $25$ (to help compress the range), and then exponentiated using Euler's number $e$ as the base. That vector is normalized so that it sums to $1$, and that represents a vector of probabilities that a particular transition will be replayed.\n",
    "\n",
    "Here, I (loosely!) draw inspiration from eligibility traces. The memory system maintains a record of all transitions that have been experienced, but gradually forgets about the older experiences depending on the free parameter $\\lambda \\in [0, 1]$. When $\\lambda = 0$, the agent is only able to remember/replay the transition it has *just* experienced (i.e., from $s_{t-1}$ to $s_t$). When $\\lambda = 1$, the agent remembers a perfect record of every transition it has ever experienced, and the probability of a given transition being replayed is directly proportional to the number of times that transition has ever been experienced. In practice, this is pretty much the same mechanism as Momennejad et al., except that the exponential decay of \"memory traces\" is dictated by a free parameter.\n",
    "\n",
    "## Prioritized sweeping\n",
    "\n",
    "The brain does not seem to randomly \"select\" events to replay, but rather prioritizes events that were especially surprising (along with those events' predecessor states). This is reminiscent of the \"prioritized sweeping\" mechanism within the Dyna architecture, in which backups propogate according to how much the backup (if performed) would change the state/action value under \"consideration.\" In practice, this method assigns prediction errors to a \"priority queue\" based on the PE magnitude. During replay, events are sequentially used to update the value estimate according to their priority in the queue. Then, for all states/actions that are predicted to lead to that event... (hm, looks like there's a model that's explicitly needed, see p240 of S&B 1e. for now, leave this alone. return to it later if you must.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 to 1.0\n1 to 1.0408107741923882\n2 to 1.0832870676749586\n3 to 1.1274968515793757\n4 to 1.1735108709918103\n5 to 1.2214027581601699\n6 to 1.2712491503214047\n7 to 1.3231298123374369\n8 to 1.3771277643359572\n9 to 1.4333294145603401\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(10):\n",
    "    print('{} to {}'.format(i, np.exp(i/25)))\n",
    "\n",
    "np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.0, 1.0408107741923882, 1.0832870676749586, 1.1274968515793757, 1.1735108709918103]\n[0.18432821041400219, 0.19185078738649505, 0.19968036654915722, 0.20782947689904818, 0.2163111587512974]\n1.0\n"
     ]
    }
   ],
   "source": [
    "y=[]\n",
    "for i in range(len(np.arange(5))):\n",
    "    y.append(np.exp(i/25)) # division: more chance to smaller numbers, avoids range error\n",
    "norm_p_weights = [float(i)/sum(y) for i in y]\n",
    "\n",
    "print(y)\n",
    "print(norm_p_weights)\n",
    "print(np.sum(norm_p_weights))"
   ]
  },
  {
   "source": [
    "# The SR in action\n",
    "\n",
    "## Description of learning problem\n",
    "\n",
    "Now that we've built all of the fundamental components of the vanilla SR agent, we can demo how it works in a very simple learning task. This is a simulation of Momennejad et al., 2017, experiment 1.\n",
    "\n",
    "Imagine that you're hungry, and that you want to go buy some lunch. You know that there are two restaurants, one to the left (three blocks away), and one to the right (also three blocks away). For some inexplicable reason, you always flip a coin when deciding which direction to go. As a learning agent, you want to learn a stable estimate of each state in your environment. How many lunches does it take before you learn a stable estimate?\n",
    "\n",
    "Momennejad, I., Russek, E. M., Cheong, J. H., Botvinick, M. M., Daw, N. D., & Gershman, S. J. (2017). The successor representation in human reinforcement learning. Nature Human Behaviour. doi:10.1038/s41562-017-0180-8\n",
    "\n",
    "## Implementing the environment\n",
    "\n",
    "In most RL implementations, state spaces are discrete, not continuous. For this reason, we'll use network graphs to simplify the problem of having to construct/navigate state spaces. We'll use the package `igraph` for this purpose. Even gridworld environments are well-described by graphs (though we don't typically think of them as such), making graphs a scalable/flexible foundation for constructing environments of arbitrary complexity."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_attrib(which_graph, which_attrib, source, target):\n",
    "    \"\"\"\n",
    "    Find the edge between two nodes, then pull out an edge attribute of interest.\n",
    "        Arguments:\n",
    "            which_graph: igraph object\n",
    "            which_attrib: string corresponding to an edge attribute\n",
    "            source: source node\n",
    "            target: target node\n",
    "        Returns:\n",
    "            attrib: the attribute of interest\n",
    "    \"\"\"\n",
    "    attrib = which_graph.es.select(_between = ([source], [target]))[which_attrib][0]\n",
    "    return attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<igraph.drawing.Plot at 0x15c776c50>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"300pt\" height=\"100pt\" viewBox=\"0 0 300 100\" version=\"1.1\">\n<defs>\n<g>\n<symbol overflow=\"visible\" id=\"glyph0-0\">\n<path style=\"stroke:none;\" d=\"M 0.453125 0 L 0.453125 -10.042969 L 8.421875 -10.042969 L 8.421875 0 Z M 7.164062 -1.257812 L 7.164062 -8.785156 L 1.710938 -8.785156 L 1.710938 -1.257812 Z M 7.164062 -1.257812 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-1\">\n<path style=\"stroke:none;\" d=\"M 6.21875 -4.117188 L 4.695312 -8.550781 L 3.078125 -4.117188 Z M 3.984375 -10.042969 L 5.523438 -10.042969 L 9.167969 0 L 7.675781 0 L 6.65625 -3.007812 L 2.6875 -3.007812 L 1.601562 0 L 0.203125 0 Z M 3.984375 -10.042969 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-2\">\n<path style=\"stroke:none;\" d=\"M 4.839844 -5.796875 C 5.414062 -5.796875 5.859375 -5.875 6.179688 -6.035156 C 6.679688 -6.285156 6.929688 -6.738281 6.929688 -7.390625 C 6.929688 -8.046875 6.664062 -8.488281 6.132812 -8.714844 C 5.832031 -8.84375 5.382812 -8.90625 4.792969 -8.90625 L 2.367188 -8.90625 L 2.367188 -5.796875 Z M 5.296875 -1.164062 C 6.132812 -1.164062 6.726562 -1.402344 7.082031 -1.886719 C 7.304688 -2.191406 7.417969 -2.5625 7.417969 -2.992188 C 7.417969 -3.722656 7.089844 -4.21875 6.4375 -4.484375 C 6.09375 -4.625 5.636719 -4.695312 5.066406 -4.695312 L 2.367188 -4.695312 L 2.367188 -1.164062 Z M 1.03125 -10.042969 L 5.34375 -10.042969 C 6.519531 -10.042969 7.359375 -9.691406 7.855469 -8.988281 C 8.148438 -8.574219 8.292969 -8.097656 8.292969 -7.554688 C 8.292969 -6.921875 8.113281 -6.402344 7.75 -5.996094 C 7.5625 -5.78125 7.296875 -5.585938 6.945312 -5.40625 C 7.460938 -5.210938 7.84375 -4.992188 8.101562 -4.742188 C 8.550781 -4.304688 8.777344 -3.703125 8.777344 -2.933594 C 8.777344 -2.285156 8.574219 -1.699219 8.167969 -1.175781 C 7.5625 -0.390625 6.597656 0 5.277344 0 L 1.03125 0 Z M 1.03125 -10.042969 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-3\">\n<path style=\"stroke:none;\" d=\"M 5.296875 -10.316406 C 6.570312 -10.316406 7.554688 -9.980469 8.257812 -9.3125 C 8.960938 -8.640625 9.347656 -7.878906 9.425781 -7.027344 L 8.101562 -7.027344 C 7.949219 -7.675781 7.652344 -8.1875 7.203125 -8.566406 C 6.753906 -8.945312 6.121094 -9.132812 5.3125 -9.132812 C 4.324219 -9.132812 3.523438 -8.785156 2.914062 -8.089844 C 2.304688 -7.394531 2.003906 -6.328125 2.003906 -4.894531 C 2.003906 -3.71875 2.277344 -2.765625 2.828125 -2.035156 C 3.375 -1.304688 4.195312 -0.9375 5.285156 -0.9375 C 6.289062 -0.9375 7.050781 -1.320312 7.574219 -2.09375 C 7.851562 -2.5 8.058594 -3.03125 8.195312 -3.691406 L 9.523438 -3.691406 C 9.40625 -2.632812 9.011719 -1.746094 8.347656 -1.03125 C 7.550781 -0.171875 6.472656 0.257812 5.121094 0.257812 C 3.953125 0.257812 2.972656 -0.09375 2.179688 -0.800781 C 1.136719 -1.734375 0.617188 -3.175781 0.617188 -5.125 C 0.617188 -6.609375 1.007812 -7.824219 1.789062 -8.769531 C 2.636719 -9.800781 3.808594 -10.316406 5.296875 -10.316406 Z M 5.296875 -10.316406 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-4\">\n<path style=\"stroke:none;\" d=\"M 4.921875 -1.164062 C 5.382812 -1.164062 5.761719 -1.210938 6.054688 -1.304688 C 6.585938 -1.484375 7.019531 -1.824219 7.355469 -2.332031 C 7.625 -2.738281 7.816406 -3.257812 7.9375 -3.890625 C 8.003906 -4.269531 8.039062 -4.617188 8.039062 -4.941406 C 8.039062 -6.1875 7.792969 -7.152344 7.296875 -7.839844 C 6.804688 -8.527344 6.007812 -8.875 4.90625 -8.875 L 2.496094 -8.875 L 2.496094 -1.164062 Z M 1.128906 -10.042969 L 5.195312 -10.042969 C 6.578125 -10.042969 7.648438 -9.550781 8.40625 -8.570312 C 9.085938 -7.6875 9.425781 -6.554688 9.425781 -5.175781 C 9.425781 -4.109375 9.226562 -3.144531 8.824219 -2.28125 C 8.117188 -0.761719 6.90625 0 5.179688 0 L 1.128906 0 Z M 1.128906 -10.042969 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-5\">\n<path style=\"stroke:none;\" d=\"M 1.195312 -10.042969 L 8.515625 -10.042969 L 8.515625 -8.8125 L 2.523438 -8.8125 L 2.523438 -5.761719 L 8.066406 -5.761719 L 8.066406 -4.601562 L 2.523438 -4.601562 L 2.523438 -1.195312 L 8.621094 -1.195312 L 8.621094 0 L 1.195312 0 Z M 1.195312 -10.042969 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-6\">\n<path style=\"stroke:none;\" d=\"M 1.195312 -10.042969 L 8.164062 -10.042969 L 8.164062 -8.8125 L 2.554688 -8.8125 L 2.554688 -5.761719 L 7.484375 -5.761719 L 7.484375 -4.566406 L 2.554688 -4.566406 L 2.554688 0 L 1.195312 0 Z M 1.195312 -10.042969 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph1-0\">\n<path style=\"stroke:none;\" d=\"M 0.386719 0 L 0.386719 -8.609375 L 7.21875 -8.609375 L 7.21875 0 Z M 6.140625 -1.078125 L 6.140625 -7.53125 L 1.464844 -7.53125 L 1.464844 -1.078125 Z M 6.140625 -1.078125 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph1-1\">\n<path style=\"stroke:none;\" d=\"M 3.246094 -8.390625 C 4.332031 -8.390625 5.117188 -7.945312 5.601562 -7.046875 C 5.976562 -6.355469 6.164062 -5.410156 6.164062 -4.207031 C 6.164062 -3.066406 5.992188 -2.125 5.65625 -1.375 C 5.164062 -0.304688 4.359375 0.226562 3.242188 0.226562 C 2.234375 0.226562 1.484375 -0.210938 0.992188 -1.085938 C 0.582031 -1.816406 0.375 -2.796875 0.375 -4.023438 C 0.375 -4.976562 0.5 -5.796875 0.742188 -6.480469 C 1.203125 -7.753906 2.039062 -8.390625 3.246094 -8.390625 Z M 3.234375 -0.734375 C 3.78125 -0.734375 4.21875 -0.976562 4.539062 -1.460938 C 4.863281 -1.945312 5.027344 -2.84375 5.027344 -4.164062 C 5.027344 -5.117188 4.910156 -5.902344 4.675781 -6.519531 C 4.441406 -7.132812 3.984375 -7.441406 3.3125 -7.441406 C 2.691406 -7.441406 2.234375 -7.148438 1.949219 -6.566406 C 1.660156 -5.980469 1.515625 -5.121094 1.515625 -3.984375 C 1.515625 -3.128906 1.609375 -2.441406 1.792969 -1.921875 C 2.074219 -1.128906 2.554688 -0.734375 3.234375 -0.734375 Z M 3.234375 -0.734375 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph1-2\">\n<path style=\"stroke:none;\" d=\"M 1.148438 -5.941406 L 1.148438 -6.75 C 1.910156 -6.824219 2.441406 -6.949219 2.742188 -7.121094 C 3.042969 -7.296875 3.265625 -7.707031 3.414062 -8.355469 L 4.25 -8.355469 L 4.25 0 L 3.125 0 L 3.125 -5.941406 Z M 1.148438 -5.941406 \"/>\n</symbol>\n</g>\n</defs>\n<g id=\"surface8\">\n<rect x=\"0\" y=\"0\" width=\"300\" height=\"100\" style=\"fill:rgb(100%,100%,100%);fill-opacity:1;stroke:none;\"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 20 20 L 125.734375 20 \"/>\n<path style=\" stroke:none;fill-rule:nonzero;fill:rgb(0%,0%,0%);fill-opacity:1;\" d=\"M 140 20 L 125.734375 24.636719 L 125.734375 15.363281 L 140 20 \"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(100%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 150 20 L 255.734375 20 \"/>\n<path style=\" stroke:none;fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;\" d=\"M 270 20 L 255.734375 24.636719 L 255.734375 15.363281 L 270 20 \"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 20 80 L 125.734375 80 \"/>\n<path style=\" stroke:none;fill-rule:nonzero;fill:rgb(0%,0%,0%);fill-opacity:1;\" d=\"M 140 80 L 125.734375 84.636719 L 125.734375 75.363281 L 140 80 \"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(100%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 150 80 L 255.734375 80 \"/>\n<path style=\" stroke:none;fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;\" d=\"M 270 80 L 255.734375 84.636719 L 255.734375 75.363281 L 270 80 \"/>\n<path style=\"fill-rule:nonzero;fill:rgb(89.803922%,89.803922%,89.803922%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 30 20 C 30 25.523438 25.523438 30 20 30 C 14.476562 30 10 25.523438 10 20 C 10 14.476562 14.476562 10 20 10 C 25.523438 10 30 14.476562 30 20 \"/>\n<path style=\"fill-rule:nonzero;fill:rgb(89.803922%,89.803922%,89.803922%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 160 20 C 160 25.523438 155.523438 30 150 30 C 144.476562 30 140 25.523438 140 20 C 140 14.476562 144.476562 10 150 10 C 155.523438 10 160 14.476562 160 20 \"/>\n<path style=\"fill-rule:nonzero;fill:rgb(89.803922%,89.803922%,89.803922%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 290 20 C 290 25.523438 285.523438 30 280 30 C 274.476562 30 270 25.523438 270 20 C 270 14.476562 274.476562 10 280 10 C 285.523438 10 290 14.476562 290 20 \"/>\n<path style=\"fill-rule:nonzero;fill:rgb(89.803922%,89.803922%,89.803922%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 30 80 C 30 85.523438 25.523438 90 20 90 C 14.476562 90 10 85.523438 10 80 C 10 74.476562 14.476562 70 20 70 C 25.523438 70 30 74.476562 30 80 \"/>\n<path style=\"fill-rule:nonzero;fill:rgb(89.803922%,89.803922%,89.803922%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 160 80 C 160 85.523438 155.523438 90 150 90 C 144.476562 90 140 85.523438 140 80 C 140 74.476562 144.476562 70 150 70 C 155.523438 70 160 74.476562 160 80 \"/>\n<path style=\"fill-rule:nonzero;fill:rgb(89.803922%,89.803922%,89.803922%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 290 80 C 290 85.523438 285.523438 90 280 90 C 274.476562 90 270 85.523438 270 80 C 270 74.476562 274.476562 70 280 70 C 285.523438 70 290 74.476562 290 80 \"/>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-1\" x=\"15.3125\" y=\"26.632812\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-2\" x=\"145.09375\" y=\"26.632812\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-3\" x=\"274.929688\" y=\"26.765625\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-4\" x=\"14.722656\" y=\"86.632812\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-5\" x=\"145.09375\" y=\"86.632812\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-6\" x=\"275.320312\" y=\"86.632812\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph1-1\" x=\"78.835938\" y=\"14.953125\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph1-2\" x=\"210.75\" y=\"14.9375\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph1-1\" x=\"78.835938\" y=\"74.953125\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph1-2\" x=\"202.164062\" y=\"74.953125\"/>\n  <use xlink:href=\"#glyph1-1\" x=\"208.837891\" y=\"74.953125\"/>\n</g>\n</g>\n</svg>\n"
     },
     "metadata": {
      "image/svg+xml": {
       "isolated": true
      }
     },
     "execution_count": 9
    }
   ],
   "source": [
    "# Environment from Momennejad et al 2017, experiment 1\n",
    "momen_edgelist = pd.DataFrame(\n",
    "    [['A', 'B', 0, 0],\n",
    "    ['B', 'C', 1, 1],\n",
    "    ['D', 'E', 0, 0],\n",
    "    ['E', 'F', 10, 1]],\n",
    "    columns = ['source', 'target', 'reward', 'terminal_state']\n",
    ")\n",
    "\n",
    "momen_nodelist = pd.DataFrame(\n",
    "    [['A', 1],\n",
    "    ['B', 0],\n",
    "    ['C', 0],\n",
    "    ['D', 1],\n",
    "    ['E', 0],\n",
    "    ['F', 0]],\n",
    "    columns = ['name', 'start_state']\n",
    ")\n",
    "\n",
    "terminal_colors = {\n",
    "    1: 'red',\n",
    "    0: 'black'\n",
    "}\n",
    "\n",
    "momen_g = ig.Graph.DataFrame(\n",
    "    edges = momen_edgelist,\n",
    "    vertices = momen_nodelist,\n",
    "    directed = True\n",
    "    )\n",
    "\n",
    "ig.plot(\n",
    "    momen_g,\n",
    "    bbox = (300, 100),\n",
    "    layout = momen_g.layout_grid(),\n",
    "    vertex_label = momen_g.vs['name'],\n",
    "    vertex_color = 'grey90',\n",
    "    edge_color = [terminal_colors[node] for node in momen_g.es['terminal_state']],\n",
    "    edge_label = momen_g.es['reward']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sr_passive(env, agent_params, replay=False, run_params=(0.001, np.Inf)):\n",
    "    \"\"\"\n",
    "    Runs a passive (no action) Successor Representation agent through an environment.\n",
    "        Arguments:\n",
    "            env: igraph Graph object\n",
    "                Edge attributes must include 'reward' (scalar) and 'terminal_state' (0/1 indicators)\n",
    "                Node attributes must include 'name' (scalar or string) and 'start_state' (0/1 indicators)\n",
    "            agent_params: numeric tuple\n",
    "                [0]: alpha\n",
    "                [1]: beta (unused)\n",
    "                [2]: gamma\n",
    "            replay: Boolean\n",
    "            run_params: tuple\n",
    "                [0]: maximum change in value estimate before concluding convergence\n",
    "                [1]: maximum number of episodes before giving up\n",
    "        Returns:\n",
    "            x\n",
    "    \"\"\"\n",
    "\n",
    "    ### Initialize\n",
    "    # Unpack agent prefs\n",
    "    alpha = agent_params[0]\n",
    "    beta = agent_params[1]\n",
    "    gamma = agent_params[2]\n",
    "\n",
    "    # Unpack runtime parameters\n",
    "    if run_params == None:\n",
    "        value_convergence_threshold = 0.001\n",
    "        max_episodes = 10000\n",
    "    else:\n",
    "        value_convergence_threshold = run_params[0]\n",
    "        max_episodes = run_params[1]\n",
    "\n",
    "    # SR matrix and weight vector\n",
    "    sr_matrix, weight_vector = init_sr(n_states = env.vcount())\n",
    "    \n",
    "    # Episodes / convergence info\n",
    "    n_episodes = 0\n",
    "    value_last_episode = np.zeros(env.vcount())\n",
    "\n",
    "    # List of valid starting state(s)\n",
    "    starting_states = [v.index for v in env.vs(start_state = 1)]\n",
    "\n",
    "    ### Run agent through experiment\n",
    "    # Repeat episodes until convergence\n",
    "    while True:\n",
    "        # Choose a starting state\n",
    "        state_now = int(np.random.choice(starting_states, size = 1))\n",
    "\n",
    "        # Move through states until terminal state is reached\n",
    "        while True:\n",
    "            # Pick a successor state randomly\n",
    "            successors = env.successors(state_now)\n",
    "            state_prev = state_now\n",
    "            state_now = int(np.random.choice(successors, size = 1))\n",
    "            reward = get_edge_attrib(env, 'reward', state_prev, state_now)\n",
    "\n",
    "            # Update SR and weights\n",
    "            agent_state = (state_prev, state_now)\n",
    "            sr_matrix = update_sr(sr_matrix, agent_params, agent_state)\n",
    "            weight_vector = update_weights(sr_matrix, agent_params, agent_state, weight_vector, reward)\n",
    "\n",
    "            # Have we reached a terminal state?\n",
    "            if get_edge_attrib(env, 'terminal_state', state_prev, state_now):\n",
    "                break\n",
    "        \n",
    "        # Update the number of episodes that have elapsed\n",
    "        n_episodes += 1\n",
    "\n",
    "        # Check convergence\n",
    "        value_this_episode = np.inner(ida_exp1_sr, ida_exp1_weights)\n",
    "        biggest_value_diff = np.max(np.abs(np.subtract(value_this_episode, value_last_episode)))\n",
    "        if biggest_value_diff < value_convergence_threshold:\n",
    "            print('Converged after {} episodes'.format(n_episodes))\n",
    "            break\n",
    "        if n_episodes == max_episodes:\n",
    "            print('Convergence failed after {} episodes'.format(n_episodes))\n",
    "            break\n",
    "        else:\n",
    "            value_last_episode = value_this_episode.copy()\n",
    "    \n",
    "    return sr_matrix, weight_vector, n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(momen_g)\n",
    "test = [v.index for v in momen_g.vs(start_state = 1)]\n",
    "\n",
    "#print(test)\n",
    "#test.index[0].index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sr_active(env_g, agent_params, replay=False, run_params=(0.001, np.Inf)):\n",
    "    ### Unpack arguments\n",
    "    alpha = agent_params[0]\n",
    "    beta = agent_params[1]\n",
    "    gamma = agent_params[2]\n",
    "    # maybe add epsilon for e-greedy, idk\n",
    "\n",
    "    active_action = agent_architecture[0]\n",
    "    replay = agent_architecture[1]\n",
    "\n",
    "    value_convergence_threshold = run_params[0]\n",
    "    max_episodes = run_params[1]\n",
    "\n",
    "    ### Initialize\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'g_ida_exp1' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f592f5936e92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize the SR matrix and weight vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mida_exp1_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mida_exp1_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_sr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_ida_exp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialize agent free parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#   (alpha, beta, gamma)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g_ida_exp1' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the SR matrix and weight vector\n",
    "ida_exp1_sr, ida_exp1_weights = init_sr(n_states = g_ida_exp1.vcount())\n",
    "\n",
    "# Initialize agent free parameters\n",
    "#   (alpha, beta, gamma)\n",
    "agent_prefs = (0.1, 0.5, 0.6)\n",
    "\n",
    "# Initialize convergence information\n",
    "value_convergence_threshold = 0.001\n",
    "n_episodes = 0\n",
    "value_last_episode = np.zeros(g_ida_exp1.vcount())\n",
    "\n",
    "# Repeat episodes until convergence\n",
    "while True:\n",
    "    # Randomly choose a starting state\n",
    "    state_now = int(np.random.choice((0, 3), size = 1))\n",
    "\n",
    "    # Move through states until terminal state is reached\n",
    "    while True:\n",
    "        # Pick a successor state\n",
    "        successors = g_ida_exp1.successors(state_now)\n",
    "        state_prev = state_now\n",
    "        state_now = int(np.random.choice(successors, size = 1))\n",
    "        reward = get_edge_attrib(g_ida_exp1, 'reward', state_prev, state_now)\n",
    "\n",
    "        # Update SR and weights\n",
    "        agent_state = [state_prev, state_now]\n",
    "        ida_exp1_sr = update_sr(ida_exp1_sr, agent_prefs, agent_state)\n",
    "        ida_exp1_weights = update_weights(ida_exp1_sr, agent_prefs, agent_state, ida_exp1_weights, reward)\n",
    "\n",
    "        # Have we reached a terminal state?\n",
    "        if get_edge_attrib(g_ida_exp1, 'terminal_state', state_prev, state_now):\n",
    "            break\n",
    "    \n",
    "    # Update the number of episodes that have elapsed\n",
    "    n_episodes += 1\n",
    "\n",
    "    # Check convergence\n",
    "    value_this_episode = np.inner(ida_exp1_sr, ida_exp1_weights)\n",
    "    biggest_value_diff = np.max(np.abs(np.subtract(value_this_episode, value_last_episode)))\n",
    "    if biggest_value_diff < value_convergence_threshold:\n",
    "        break\n",
    "    else:\n",
    "        value_last_episode = value_this_episode.copy()\n",
    "\n",
    "# Plot results\n",
    "plot_sr, ax_sr = plt.subplots(1)\n",
    "sns.heatmap(ida_exp1_sr, annot=True, square=True, cmap=\"YlGnBu\", ax=ax_sr).set_title('Converged SR matrix')\n",
    "\n",
    "plot_reward, ax_reward = plt.subplots(1)\n",
    "sns.heatmap(ida_exp1_weights[np.newaxis, :], annot=True, cmap=\"YlGnBu\", ax=ax_reward).set_title('Converged (reward) weight vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(ida_exp1_sr, annot=True, square=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(ida_exp1_weights[np.newaxis, :], annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "source": [
    "# sandbox"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Convergence failed after 5 attempts\n"
     ]
    }
   ],
   "source": [
    "test = 5\n",
    "print('Convergence failed after {} attempts'.format(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}